# AlexNet

### 开发环境(Development Environment 개발환경)
- PyCharm
- Python
- Torch

### 发展背景

AlexNet由Alex Krizhevsky于2012年提出，夺得2012年ILSVRC比赛的冠军，top5预测的错误率为16.4%，它以领先第二名10%的准确率夺得冠军，并且成功的向世界展示了深度学习的魅力。

AlexNet创新点

1. 使用ReLU作为CNN的激活函数，验证了其效果在较深的网络中超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。（ReLU激活函数很早就被提出，却没有被发扬光大。
2. 使用重叠的最大池化（步长小于卷积核），即在池化的时候，每次移动的步长小于池化的边长。重叠效果提升特征的丰富性，降低了top-1和top-5的错误率，同时缓解了过拟合。
3. 局部响应归一化（LRN），对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
4. 训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。

### 主要原理(주요원리)

1. 卷积池化层原理

该层的顺序是：卷积——>ReLU——>池化——>归一化。

卷积池化层原理：

根据神经元公式：h(x)=f(wx+b）

上式子就是神经元所表示的函数，x表示输入（图片矩阵），w表示权重（卷积核参数），b表示偏置，f表示激活函数（ReLU），h(x)表示输出，输出后的数据经过池化，然后归一化。

形成完成的卷积池化层。

训练卷积神经网络的过程就是不断调整权重w（卷积核参数）与偏置b的过程，以使其输出h(x)达到预期值。

首先对输入的图片与卷积核w进行运算，并且每个卷积核参加卷积运算时添加偏置b，然后用激活函数处理卷积运算结果，激活后的图片尺寸不变，然后再进行池化运算，池化运算可以降低图片尺寸，然后对池化后的结果进行归一化处理。

2. 全连接层原理



- 这个项目是我为了重新学习AlexNet而做的项目（이 프로젝트는 내가 AlexNet를 다시 공부하기위해서 만든 프로젝트입니다.）
