# AlexNet

### 开发环境(Development Environment 개발환경)
- PyCharm
- Python
- Torch


### 发展背景

AlexNet由Alex Krizhevsky于2012年提出，夺得2012年ILSVRC比赛的冠军，top5预测的错误率为16.4%，它以领先第二名10%的准确率夺得冠军，并且成功的向世界展示了深度学习的魅力。

AlexNet创新点

1. 使用ReLU作为CNN的激活函数，验证了其效果在较深的网络中超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。（ReLU激活函数很早就被提出，却没有被发扬光大。
2. 使用重叠的最大池化（步长小于卷积核），即在池化的时候，每次移动的步长小于池化的边长。重叠效果提升特征的丰富性，降低了top-1和top-5的错误率，同时缓解了过拟合。
3. 局部响应归一化（LRN），对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
4. 训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。

### 主要原理(주요원리)

1. 卷积池化层原理

该层的顺序是：卷积——>ReLU——>池化——>归一化。

卷积池化层原理：

根据神经元公式：h(x)=f(wx+b)

上式子就是神经元所表示的函数，x表示输入（图片矩阵），w表示权重（卷积核参数），b表示偏置，f表示激活函数（ReLU），h(x)表示输出，输出后的数据经过池化，然后归一化。

形成完成的卷积池化层。

训练卷积神经网络的过程就是不断调整权重w（卷积核参数）与偏置b的过程，以使其输出h(x)达到预期值。

首先对输入的图片与卷积核w进行运算，并且每个卷积核参加卷积运算时添加偏置b，然后用激活函数处理卷积运算结果，激活后的图片尺寸不变，然后再进行池化运算，池化运算可以降低图片尺寸，然后对池化后的结果进行归一化处理。

2. 全连接层原理

![20190310171644852](https://user-images.githubusercontent.com/60682087/132558041-a1d593a5-4a04-4edc-8cf6-edf59b76d6f3.jpg)

根据神经元公式：h(x)=f(wx+b) 全连接层x表示上一层的输出值，w表示本层的神经元权重，权重数量等于本全连接层神经元数目，然后再加偏置b，偏置数量等于本层神经元数目。

3. 模型参数详解

原始论文中的图如下：网络模型由5个卷积池化层，3个全连接层组成，最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。

![20190309171443497](https://user-images.githubusercontent.com/60682087/132558491-b394c2a5-4c0b-4ffa-b47e-b9b5e2f03dd5.jpg)

为了更加清晰的说明参数，引入如下更全的网络结构。

![20190309170611156](https://user-images.githubusercontent.com/60682087/132558696-b2c1726d-d5eb-42e7-990f-7c50c539b68f.jpg)

![20190309172535933](https://user-images.githubusercontent.com/60682087/132558756-1dab4ea8-7196-44c8-9fed-937481949f1c.jpg)

1. 卷积池化层1

![20190309210938354](https://user-images.githubusercontent.com/60682087/132558867-cbdfa05d-08d7-4eae-bb01-dbef147ad97d.jpg)

![1-1](https://user-images.githubusercontent.com/60682087/132558925-eab368d6-a2b0-4252-a2b6-0b73af1a2ff2.JPG)

（1）卷积运算

第一层输入数据为原始的227x227x3的图像，这个图像被11x11x3的卷积核进行卷积运算，卷积核对原始图像的每次卷积都生成一个新的像素。卷积核沿原始图像的x轴方向和y轴方向两个方向移动，移动的步长是4个像素。因此，卷积核在移动的过程中会生成(227-11)/4+1=55个像素，行和列的55x55个像素形成对原始图像卷积之后的像素层。共有96个卷积核，会生成55x55x96个卷积后的像素层。

（2）分组

96个卷积核分成2组，每组48个卷积核。对应生成2组55x55x48的卷积后的像素层数据。

（3）激活函数层

这些像素层经过relu1单元的处理，生成激活像素层，尺寸仍为2组55x55x48的像素层数据。

（4）池化层

经过激活函数的像素层经过pool运算的处理，池化运算的尺度为3x3，运算的步长为2，则池化后图像的尺寸为(55-3)/2+1=27。 即池化后像素的规模为27x27x96。

（5）归一化处理

归一化运算的尺度为5x5，即选择5x5的像素区域进行批归一化处理。第一卷积层运算结束后形成的像素层的规模为27x27x96。分别对应96个卷积核所运算形成。这96层像素层分为2组，每组48个像素层，每组在一个独立的GPU上进行运算。

（6）参数数量

卷积层的参数 = 卷积核大小 x 卷积核的数量 + 偏置数量(即卷积核的数量)。
本层参数数量为: (11 x 11 x 3 x 96) + 96 = 34848， 注：参数分为2部分 w 和 b ，“+”前面一部分是 w 的数量, “+”后面那部分是 b 的数量，后面的层也按这个思路来计算。

2. 卷积池化层2

![20190309211250937](https://user-images.githubusercontent.com/60682087/132559661-a0d7e7d6-8c6e-4a4d-a18e-9c13894bc4fd.jpg)

![2-2](https://user-images.githubusercontent.com/60682087/132559712-bff22a98-1d76-40a3-ba67-aa5dc2cbd38b.JPG)

（1）卷积运算

第二层输入数据为第一层输出的27x27x96的像素层，为便于后续处理，每幅像素层的左右两边和上下两边都要填充2个像素；27x27x96的像素数据分成272748的两组像素数据，两组数据分别再两个不同的GPU中进行运算。每组像素数据被5548的卷积核进行卷积运算，共有256个5x5x48卷积核。卷积核沿原始图像的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，卷积核在移动的过程中会生成(27-5+2x2)/1+1=27个像素。即会生成27x27x256个卷积后的像素层。

（2）分组

这256个卷积核分成两组。会生成两组27x27x128个卷积后的像素层。

（3）激活函数层

这些像素层经过relu2单元的处理，生成激活像素层，尺寸仍为两组27x27x128的像素层。

（4）池化层

这些像素层经过pool运算(池化运算)的处理，池化运算的尺度为3*3，运算的步长为2，则池化后图像的尺寸为(57-3)/2+1=13。 即池化后像素的规模为2组13x13x128的像素层；

（5）归一化处理

然后经过归一化处理，归一化运算的尺度为5x5，即选择5x5的像素区域进行批归一化处理。第二卷积层运算结束后形成的像素层的规模为2组13x13x128的像素层。分别对应2组128个卷积核所运算形成。

（6）参数数量

卷积层, 使用256个5x5x48卷积核，节点数量：27x27x128x2 = 186624，参数数量：(5x5x48x128+128)x2 = 307456，最后"*2"是因为网络层均匀分布在两个GPU上，分2组，先计算单个GPU上的参数，再乘以GPU数量2。

3. 卷积层3

![3-3](https://user-images.githubusercontent.com/60682087/132560324-82b9d5bc-6c8c-42ef-8a51-866de7d1ef2f.JPG)

（1）卷积运算

第三层输入数据为第二层输出的2组13x13x128的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是3x3x256。因此，每个GPU中的卷积核都能对2组13x13x128的像素层的所有数据进行卷积运算。

（2）分组

这384个卷积核分成两组。会生成两组13x13x192个卷积后的像素层。

（3）激活层

这些像素层经过relu3单元的处理，生成激活像素层，尺寸仍为2组13x13x192像素层，共13x13x384个像素层。

（4）参数数量

卷积层，使用384个3x3x256卷积核，节点数量：13x13x192x2 = 64896，参数数量：3x3x256x384+384 = 885120。

4. 卷积层4

![20190309213124661](https://user-images.githubusercontent.com/60682087/132560852-3acd1f5a-a965-4687-808c-53bec4ba76b2.jpg)

![4-4](https://user-images.githubusercontent.com/60682087/132560884-6a2d0174-4c30-4768-be34-afcc362112b4.JPG)

（1）卷积运算

第四层输入数据为第三层输出的2组13x13x192的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是3x3x192。因此，每个GPU中的卷积核能对1组13x13x192的像素层的数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为(13-3+12)/1+1=13，2个GPU中共1313*384个卷积后的像素层。即与上一层相同。

（2）分组

这384个卷积核分成两组。会生成两组13x13x192个卷积后的像素层。

（3）激活层

这些像素层经过relu4单元的处理，生成激活像素层，尺寸仍为2组13x13x192像素层，共13x13x384个像素层。

（4）参数数量

卷积层，使用384个3x3x192卷积核，分2组，节点数量：13x13x192x2 = 64896，参数数量：(3x3x192x192+192)x2 = 663936。

5. 卷积池化层5

![20190309213146630](https://user-images.githubusercontent.com/60682087/132561094-002576ac-b2ec-4ec6-9bf5-1333899c5bc6.jpg)

![5-5](https://user-images.githubusercontent.com/60682087/132561106-ba495680-d61c-4e2c-bcf1-4f48b38fe88f.JPG)

（1）卷积运算

第五层输入数据为第四层输出的2组13x13x192的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有128个卷积核，每个卷积核的尺寸是3x3x192。因此，每个GPU中的卷积核能对1组13x13x192的像素层的数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的x轴方向和y轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为(13-3+1x2)/1+1=13，每个GPU中共13x13x128个卷积核。

（2）分组

分组为2组13x13x128像素层，共13x13x256个像素层。

（3）激活函数

这些像素层经过relu5单元的处理，生成激活像素层，尺寸仍为2组13x13x128像素层，共13x13x256个像素层。

（4）池化层

2组13x13x128像素层分别在2个不同GPU中进行池化(pool)运算处理。池化运算的尺度为3x3，运算的步长为2，则池化后图像的尺寸为(13-3)/2+1=6。 即池化后像素的规模为两组6x6x128的像素层数据，共6x6x256规模的像素层数据。

（5）参数数量

卷积层，使用256个3x3x192卷积核，分2组，节点数量：13x13x128x2 = 43264，参数数量：(3x3x192x128+128)x2 = 442624。

6. 全连接层1

![201903092131466gg30](https://user-images.githubusercontent.com/60682087/132561311-ac632a10-f911-4de7-981b-0113d78e8ead.jpg)

![6-6](https://user-images.githubusercontent.com/60682087/132561319-5672278a-2806-48fb-a87c-7f62f6f35c15.JPG)

（1）卷积运算

第六层输入数据的尺寸是6x6x256，采用6x6x256=9216尺寸的滤波器对第六层的输入数据进行卷积运算；每个6x6x256尺寸的滤波器对第六层的输入数据进行卷积运算生成一个运算结果，通过一个神经元输出这个运算结果；共有4096个6x6x256尺寸的滤波器对输入数据进行卷积运算，通过4096个神经元输出运算结果；

（2）激活函数

这4096个运算结果通过relu6激活函数生成4096个值。

（3）Dropout

可以看到卷积神经网络的计算量其实主要集中在全连接层，这些层参数太多了，也最容易发生过拟合。DropOut通过训练时随机使得一部分结点失效，不贡献连接权重而减少过拟合风险。同时强迫这些神经元去学习互补的一些特征。因此最后通过dropout6运算后输出4096个本层的输出结果值。

（4）参数数量

根据全连接层的参数数量 = 上一层节点数量(pooling之后的) x 下一层节点数量 + 偏置数量（即下一层的节点数量）。
参数数量为：(66128*2)*4096+4096 = 37752832，可以看到这个参数数量远远大于之前所有卷积层的参数数量之和。也就是说AlexNet的参数大部分位于后面的全连接层。

7. 全连接层2

![201903092131466gggg30](https://user-images.githubusercontent.com/60682087/132561644-00db2655-13f4-48c7-bc96-79ce73bdda8a.jpg)

![7-7](https://user-images.githubusercontent.com/60682087/132561652-27bbd002-5621-4583-9814-6912de5091d7.JPG)

（1）卷积运算

第六层输出的4096个数据与第七层的4096个神经元进行全连接

（2）激活函数

然后经由relu7进行处理后生成4096个数据

（3）Dropout

再经过dropout7处理后输出4096个数据。

（4）参数数量

全连接层，节点数量为4096。参数数量为：4096*4096 + 4096= 16781312。

8. 全连接层3

![201903092131466ggggggg30](https://user-images.githubusercontent.com/60682087/132561858-16517a15-bc9b-4967-83f4-e77b8de5a46a.jpg)

![8-8](https://user-images.githubusercontent.com/60682087/132561865-aac43218-f0a7-4fac-83e3-e5d48bddbc19.JPG)

（1）卷积运算

第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出被训练的数值。

（2）参数数量

全连接层，节点数量为1000。参数数量为： 4096*1000 + 1000 = 4097000。

输出：

<pre><code>
AlexNet(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU()
  )
  (max_pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU(inplace=True)
  )
  (max_pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  (fc1): Sequential(
    (0): Linear(in_features=1024, out_features=384, bias=True)
    (1): ReLU(inplace=True)
  )
  (fc2): Sequential(
    (0): Linear(in_features=384, out_features=192, bias=True)
    (1): ReLU(inplace=True)
  )
  (fc3): Linear(in_features=192, out_features=10, bias=True)
)
</code></pre>

输入图像大小(1, 3, 32, 32)，验证模型输出大小。

<pre><code>
input_demo = Variable(torch.zeros(1, 3, 32, 32))
output_demo = alexnet(input_demo)
print(output_demo.shape)
</code></pre>

- 这个项目是我为了重新学习AlexNet而做的项目（이 프로젝트는 내가 AlexNet를 다시 공부하기위해서 만든 프로젝트입니다.）
